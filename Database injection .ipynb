{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5247ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending data to database \n",
    "#importing pandas , os , sqlalchemy \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "import time\n",
    "\n",
    "logging.basicConfig(\n",
    "   filename=\"logs/ingestion_db.log\",\n",
    "   level=logging.DEBUG,\n",
    "   format=\"%(asctime)s- %(levelname)s- %(message)s\",\n",
    "   filemode='a'\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "#Create the engine\n",
    "engine=create_engine('mysql+pymysql://root:root@localhost:3306/inventory')\n",
    "\n",
    "\n",
    "#os.getcwd()    \n",
    "folder_path='C:\\\\Users\\\\Prathamesh\\\\Vendor Performance Analysis'    #created the current folder path\n",
    "\n",
    "def load_raw_data():\n",
    "    start=time.time()\n",
    "    for file in os.lisdir(folder_path):\n",
    "         if '.csv' in file:\n",
    "                        file_path=os.path.join(folder_path,file)\n",
    "                        df=pd.read_csv(file_path)\n",
    "                        logging.info(f'Ingesting {file} in db')\n",
    "                        ingest_db(df,file[:-4],engine)\n",
    "    \n",
    "    end=time.time()\n",
    "    total_time=(end-start)/60\n",
    "    logging.info(f'Time taken for ingestion -{total_time}')\n",
    "    logging.info('Ingestion Cmplete')\n",
    "                        \n",
    "       \n",
    "   \n",
    "if __name__=__main__:\n",
    "    load_raw_data()\n",
    "\n",
    "\n",
    "def ingest_db(df,table_name,engine):\n",
    "    '''this function will ingest the dataframe in the database with required tablename with help of connection established through engine'''\n",
    "    df.to_sql(table_name,con=engine,if_exists='replace',index=False)\n",
    "    \n",
    "#if_exists='replace'\tIf a table with that name already exists: delete it and create a new one\n",
    "\n",
    "def load_raw_data():\n",
    "    '''this function will load the CSVs as Dataframes and ingest into database'''\n",
    "    start=time.time()\n",
    "    for file in os.listdir(folder_path):\n",
    "    if '.csv' in file:\n",
    "        file_path=os.path.join(folder_path,file)\n",
    "        df=pd.read_csv(file_path)\n",
    "        logging.info(f'Ingesting {file} in db')\n",
    "        ingest_db(df,file[:-4],engine)\n",
    "    end=time.time()\n",
    "    total_time=(end-start)/60             #because it provides in seconds we need in minutes so divide by 60\n",
    "    \n",
    "    logging.info('-----------------Ingestion Complete------------------')\n",
    "    logging.info(f'\\nTotal time taken : {total_time} minutes')\n",
    "    \n",
    "if __name__=\"__main__\":\n",
    "    load_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a1c7c",
   "metadata": {},
   "source": [
    "Using Python’s built-in logging module —for tracking what code is doing\n",
    "\n",
    "Breakdown of what logging.basicConfig() line does:\n",
    "\n",
    "\n",
    "Parameter\tWhat it does\n",
    "\n",
    "filename=\"logs/ingestion_db.log\"\tTells Python to save all log messages to a file called ingestion_db.log inside a logs folder\n",
    "\n",
    "level=logging.DEBUG \tSet the logging level to DEBUG, so it captures everything (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "\n",
    "format=\"%(asctime)s - %(levelname)s - %(message)s\"  \tDefines how each log line will look: it includes the timestamp, log level, and the actual message\n",
    "    \n",
    "filemode='a'\tOpens the file in append mode — so new logs will be added, not overwrite the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa87f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file=(\"C:\\\\Users\\\\Prathamesh\\\\Vendor Performance Analysis\\\\vendor_invoice.csv\")\n",
    "df=pd.read_csv(\"C:\\\\Users\\\\Prathamesh\\\\Vendor Performance Analysis\\\\vendor_invoice.csv\")\n",
    "#df.head(2)\n",
    "start1=time.time()\n",
    "ingest_db(df,'vendor_invoice',engine)\n",
    "logging.info(f'Ingesting {file} in db')\n",
    "logging.info('-----------------Ingestion Complete------------------')\n",
    "end1=time.time()\n",
    "total_time_sales=(end1-start1)/60\n",
    "logging.info(f'\\nTotal time taken : {total_time_sales} minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c60aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f010990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingested chunk 1 with 50000 rows.\n",
      "✅ Ingested chunk 2 with 50000 rows.\n",
      "✅ Ingested chunk 3 with 50000 rows.\n",
      "✅ Ingested chunk 4 with 50000 rows.\n",
      "✅ Ingested chunk 5 with 50000 rows.\n",
      "✅ Ingested chunk 6 with 50000 rows.\n",
      "✅ Ingested chunk 7 with 50000 rows.\n",
      "✅ Ingested chunk 8 with 50000 rows.\n",
      "✅ Ingested chunk 9 with 50000 rows.\n",
      "✅ Ingested chunk 10 with 50000 rows.\n",
      "✅ Ingested chunk 11 with 50000 rows.\n",
      "✅ Ingested chunk 12 with 50000 rows.\n",
      "✅ Ingested chunk 13 with 50000 rows.\n",
      "✅ Ingested chunk 14 with 50000 rows.\n",
      "✅ Ingested chunk 15 with 50000 rows.\n",
      "✅ Ingested chunk 16 with 50000 rows.\n",
      "✅ Ingested chunk 17 with 50000 rows.\n",
      "✅ Ingested chunk 18 with 50000 rows.\n",
      "✅ Ingested chunk 19 with 50000 rows.\n",
      "✅ Ingested chunk 20 with 50000 rows.\n",
      "✅ Ingested chunk 21 with 50000 rows.\n",
      "✅ Ingested chunk 22 with 50000 rows.\n",
      "✅ Ingested chunk 23 with 50000 rows.\n",
      "✅ Ingested chunk 24 with 50000 rows.\n",
      "✅ Ingested chunk 25 with 50000 rows.\n",
      "✅ Ingested chunk 26 with 50000 rows.\n",
      "✅ Ingested chunk 27 with 50000 rows.\n",
      "✅ Ingested chunk 28 with 50000 rows.\n",
      "✅ Ingested chunk 29 with 50000 rows.\n",
      "✅ Ingested chunk 30 with 50000 rows.\n",
      "✅ Ingested chunk 31 with 50000 rows.\n",
      "✅ Ingested chunk 32 with 50000 rows.\n",
      "✅ Ingested chunk 33 with 50000 rows.\n",
      "✅ Ingested chunk 34 with 50000 rows.\n",
      "✅ Ingested chunk 35 with 50000 rows.\n",
      "✅ Ingested chunk 36 with 50000 rows.\n",
      "✅ Ingested chunk 37 with 50000 rows.\n",
      "✅ Ingested chunk 38 with 50000 rows.\n",
      "✅ Ingested chunk 39 with 50000 rows.\n",
      "✅ Ingested chunk 40 with 50000 rows.\n",
      "✅ Ingested chunk 41 with 50000 rows.\n",
      "✅ Ingested chunk 42 with 50000 rows.\n",
      "✅ Ingested chunk 43 with 50000 rows.\n",
      "✅ Ingested chunk 44 with 50000 rows.\n",
      "✅ Ingested chunk 45 with 50000 rows.\n",
      "✅ Ingested chunk 46 with 50000 rows.\n",
      "✅ Ingested chunk 47 with 50000 rows.\n",
      "✅ Ingested chunk 48 with 50000 rows.\n",
      "✅ Ingested chunk 49 with 50000 rows.\n",
      "✅ Ingested chunk 50 with 50000 rows.\n",
      "✅ Ingested chunk 51 with 50000 rows.\n",
      "✅ Ingested chunk 52 with 50000 rows.\n",
      "✅ Ingested chunk 53 with 50000 rows.\n",
      "✅ Ingested chunk 54 with 50000 rows.\n",
      "✅ Ingested chunk 55 with 50000 rows.\n",
      "✅ Ingested chunk 56 with 50000 rows.\n",
      "✅ Ingested chunk 57 with 50000 rows.\n",
      "✅ Ingested chunk 58 with 50000 rows.\n",
      "✅ Ingested chunk 59 with 50000 rows.\n",
      "✅ Ingested chunk 60 with 50000 rows.\n",
      "✅ Ingested chunk 61 with 50000 rows.\n",
      "✅ Ingested chunk 62 with 50000 rows.\n",
      "✅ Ingested chunk 63 with 50000 rows.\n",
      "✅ Ingested chunk 64 with 50000 rows.\n",
      "✅ Ingested chunk 65 with 50000 rows.\n",
      "✅ Ingested chunk 66 with 50000 rows.\n",
      "✅ Ingested chunk 67 with 50000 rows.\n",
      "✅ Ingested chunk 68 with 50000 rows.\n",
      "✅ Ingested chunk 69 with 50000 rows.\n",
      "✅ Ingested chunk 70 with 50000 rows.\n",
      "✅ Ingested chunk 71 with 50000 rows.\n",
      "✅ Ingested chunk 72 with 50000 rows.\n",
      "✅ Ingested chunk 73 with 50000 rows.\n",
      "✅ Ingested chunk 74 with 50000 rows.\n",
      "✅ Ingested chunk 75 with 50000 rows.\n",
      "✅ Ingested chunk 76 with 50000 rows.\n",
      "✅ Ingested chunk 77 with 50000 rows.\n",
      "✅ Ingested chunk 78 with 50000 rows.\n",
      "✅ Ingested chunk 79 with 50000 rows.\n",
      "✅ Ingested chunk 80 with 50000 rows.\n",
      "✅ Ingested chunk 81 with 50000 rows.\n",
      "✅ Ingested chunk 82 with 50000 rows.\n",
      "✅ Ingested chunk 83 with 50000 rows.\n",
      "✅ Ingested chunk 84 with 50000 rows.\n",
      "✅ Ingested chunk 85 with 50000 rows.\n",
      "✅ Ingested chunk 86 with 50000 rows.\n",
      "✅ Ingested chunk 87 with 50000 rows.\n",
      "✅ Ingested chunk 88 with 50000 rows.\n",
      "✅ Ingested chunk 89 with 50000 rows.\n",
      "✅ Ingested chunk 90 with 50000 rows.\n",
      "✅ Ingested chunk 91 with 50000 rows.\n",
      "✅ Ingested chunk 92 with 50000 rows.\n",
      "✅ Ingested chunk 93 with 50000 rows.\n",
      "✅ Ingested chunk 94 with 50000 rows.\n",
      "✅ Ingested chunk 95 with 50000 rows.\n",
      "✅ Ingested chunk 96 with 50000 rows.\n",
      "✅ Ingested chunk 97 with 50000 rows.\n",
      "✅ Ingested chunk 98 with 50000 rows.\n",
      "✅ Ingested chunk 99 with 50000 rows.\n",
      "✅ Ingested chunk 100 with 50000 rows.\n",
      "✅ Ingested chunk 101 with 50000 rows.\n",
      "✅ Ingested chunk 102 with 50000 rows.\n",
      "✅ Ingested chunk 103 with 50000 rows.\n",
      "✅ Ingested chunk 104 with 50000 rows.\n",
      "✅ Ingested chunk 105 with 50000 rows.\n",
      "✅ Ingested chunk 106 with 50000 rows.\n",
      "✅ Ingested chunk 107 with 50000 rows.\n",
      "✅ Ingested chunk 108 with 50000 rows.\n",
      "✅ Ingested chunk 109 with 50000 rows.\n",
      "✅ Ingested chunk 110 with 50000 rows.\n",
      "✅ Ingested chunk 111 with 50000 rows.\n",
      "✅ Ingested chunk 112 with 50000 rows.\n",
      "✅ Ingested chunk 113 with 50000 rows.\n",
      "✅ Ingested chunk 114 with 50000 rows.\n",
      "✅ Ingested chunk 115 with 50000 rows.\n",
      "✅ Ingested chunk 116 with 50000 rows.\n",
      "✅ Ingested chunk 117 with 50000 rows.\n",
      "✅ Ingested chunk 118 with 50000 rows.\n",
      "✅ Ingested chunk 119 with 50000 rows.\n",
      "✅ Ingested chunk 120 with 50000 rows.\n",
      "✅ Ingested chunk 121 with 50000 rows.\n",
      "✅ Ingested chunk 122 with 50000 rows.\n",
      "✅ Ingested chunk 123 with 50000 rows.\n",
      "✅ Ingested chunk 124 with 50000 rows.\n",
      "✅ Ingested chunk 125 with 50000 rows.\n",
      "✅ Ingested chunk 126 with 50000 rows.\n",
      "✅ Ingested chunk 127 with 50000 rows.\n",
      "✅ Ingested chunk 128 with 50000 rows.\n",
      "✅ Ingested chunk 129 with 50000 rows.\n",
      "✅ Ingested chunk 130 with 50000 rows.\n",
      "✅ Ingested chunk 131 with 50000 rows.\n",
      "✅ Ingested chunk 132 with 50000 rows.\n",
      "✅ Ingested chunk 133 with 50000 rows.\n",
      "✅ Ingested chunk 134 with 50000 rows.\n",
      "✅ Ingested chunk 135 with 50000 rows.\n",
      "✅ Ingested chunk 136 with 50000 rows.\n",
      "✅ Ingested chunk 137 with 50000 rows.\n",
      "✅ Ingested chunk 138 with 50000 rows.\n",
      "✅ Ingested chunk 139 with 50000 rows.\n",
      "✅ Ingested chunk 140 with 50000 rows.\n",
      "✅ Ingested chunk 141 with 50000 rows.\n",
      "✅ Ingested chunk 142 with 50000 rows.\n",
      "✅ Ingested chunk 143 with 50000 rows.\n",
      "✅ Ingested chunk 144 with 50000 rows.\n",
      "✅ Ingested chunk 145 with 50000 rows.\n",
      "✅ Ingested chunk 146 with 50000 rows.\n",
      "✅ Ingested chunk 147 with 50000 rows.\n",
      "✅ Ingested chunk 148 with 50000 rows.\n",
      "✅ Ingested chunk 149 with 50000 rows.\n",
      "✅ Ingested chunk 150 with 50000 rows.\n",
      "✅ Ingested chunk 151 with 50000 rows.\n",
      "✅ Ingested chunk 152 with 50000 rows.\n",
      "✅ Ingested chunk 153 with 50000 rows.\n",
      "✅ Ingested chunk 154 with 50000 rows.\n",
      "✅ Ingested chunk 155 with 50000 rows.\n",
      "✅ Ingested chunk 156 with 50000 rows.\n",
      "✅ Ingested chunk 157 with 50000 rows.\n",
      "✅ Ingested chunk 158 with 50000 rows.\n",
      "✅ Ingested chunk 159 with 50000 rows.\n",
      "✅ Ingested chunk 160 with 50000 rows.\n",
      "✅ Ingested chunk 161 with 50000 rows.\n",
      "✅ Ingested chunk 162 with 50000 rows.\n",
      "✅ Ingested chunk 163 with 50000 rows.\n",
      "✅ Ingested chunk 164 with 50000 rows.\n",
      "✅ Ingested chunk 165 with 50000 rows.\n",
      "✅ Ingested chunk 166 with 50000 rows.\n",
      "✅ Ingested chunk 167 with 50000 rows.\n",
      "✅ Ingested chunk 168 with 50000 rows.\n",
      "✅ Ingested chunk 169 with 50000 rows.\n",
      "✅ Ingested chunk 170 with 50000 rows.\n",
      "✅ Ingested chunk 171 with 50000 rows.\n",
      "✅ Ingested chunk 172 with 50000 rows.\n",
      "✅ Ingested chunk 173 with 50000 rows.\n",
      "✅ Ingested chunk 174 with 50000 rows.\n",
      "✅ Ingested chunk 175 with 50000 rows.\n",
      "✅ Ingested chunk 176 with 50000 rows.\n",
      "✅ Ingested chunk 177 with 50000 rows.\n",
      "✅ Ingested chunk 178 with 50000 rows.\n",
      "✅ Ingested chunk 179 with 50000 rows.\n",
      "✅ Ingested chunk 180 with 50000 rows.\n",
      "✅ Ingested chunk 181 with 50000 rows.\n",
      "✅ Ingested chunk 182 with 50000 rows.\n",
      "✅ Ingested chunk 183 with 50000 rows.\n",
      "✅ Ingested chunk 184 with 50000 rows.\n",
      "✅ Ingested chunk 185 with 50000 rows.\n",
      "✅ Ingested chunk 186 with 50000 rows.\n",
      "✅ Ingested chunk 187 with 50000 rows.\n",
      "✅ Ingested chunk 188 with 50000 rows.\n",
      "✅ Ingested chunk 189 with 50000 rows.\n",
      "✅ Ingested chunk 190 with 50000 rows.\n",
      "✅ Ingested chunk 191 with 50000 rows.\n",
      "✅ Ingested chunk 192 with 50000 rows.\n",
      "✅ Ingested chunk 193 with 50000 rows.\n",
      "✅ Ingested chunk 194 with 50000 rows.\n",
      "✅ Ingested chunk 195 with 50000 rows.\n",
      "✅ Ingested chunk 196 with 50000 rows.\n",
      "✅ Ingested chunk 197 with 50000 rows.\n",
      "✅ Ingested chunk 198 with 50000 rows.\n",
      "✅ Ingested chunk 199 with 50000 rows.\n",
      "✅ Ingested chunk 200 with 50000 rows.\n",
      "✅ Ingested chunk 201 with 50000 rows.\n",
      "✅ Ingested chunk 202 with 50000 rows.\n",
      "✅ Ingested chunk 203 with 50000 rows.\n",
      "✅ Ingested chunk 204 with 50000 rows.\n",
      "✅ Ingested chunk 205 with 50000 rows.\n",
      "✅ Ingested chunk 206 with 50000 rows.\n",
      "✅ Ingested chunk 207 with 50000 rows.\n",
      "✅ Ingested chunk 208 with 50000 rows.\n",
      "✅ Ingested chunk 209 with 50000 rows.\n",
      "✅ Ingested chunk 210 with 50000 rows.\n",
      "✅ Ingested chunk 211 with 50000 rows.\n",
      "✅ Ingested chunk 212 with 50000 rows.\n",
      "✅ Ingested chunk 213 with 50000 rows.\n",
      "✅ Ingested chunk 214 with 50000 rows.\n",
      "✅ Ingested chunk 215 with 50000 rows.\n",
      "✅ Ingested chunk 216 with 50000 rows.\n",
      "✅ Ingested chunk 217 with 50000 rows.\n",
      "✅ Ingested chunk 218 with 50000 rows.\n",
      "✅ Ingested chunk 219 with 50000 rows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingested chunk 220 with 50000 rows.\n",
      "✅ Ingested chunk 221 with 50000 rows.\n",
      "✅ Ingested chunk 222 with 50000 rows.\n",
      "✅ Ingested chunk 223 with 50000 rows.\n",
      "✅ Ingested chunk 224 with 50000 rows.\n",
      "✅ Ingested chunk 225 with 50000 rows.\n",
      "✅ Ingested chunk 226 with 50000 rows.\n",
      "✅ Ingested chunk 227 with 50000 rows.\n",
      "✅ Ingested chunk 228 with 50000 rows.\n",
      "✅ Ingested chunk 229 with 50000 rows.\n",
      "✅ Ingested chunk 230 with 50000 rows.\n",
      "✅ Ingested chunk 231 with 50000 rows.\n",
      "✅ Ingested chunk 232 with 50000 rows.\n",
      "✅ Ingested chunk 233 with 50000 rows.\n",
      "✅ Ingested chunk 234 with 50000 rows.\n",
      "✅ Ingested chunk 235 with 50000 rows.\n",
      "✅ Ingested chunk 236 with 50000 rows.\n",
      "✅ Ingested chunk 237 with 50000 rows.\n",
      "✅ Ingested chunk 238 with 50000 rows.\n",
      "✅ Ingested chunk 239 with 50000 rows.\n",
      "✅ Ingested chunk 240 with 50000 rows.\n",
      "✅ Ingested chunk 241 with 50000 rows.\n",
      "✅ Ingested chunk 242 with 50000 rows.\n",
      "✅ Ingested chunk 243 with 50000 rows.\n",
      "✅ Ingested chunk 244 with 50000 rows.\n",
      "✅ Ingested chunk 245 with 50000 rows.\n",
      "✅ Ingested chunk 246 with 50000 rows.\n",
      "✅ Ingested chunk 247 with 50000 rows.\n",
      "✅ Ingested chunk 248 with 50000 rows.\n",
      "✅ Ingested chunk 249 with 50000 rows.\n",
      "✅ Ingested chunk 250 with 50000 rows.\n",
      "✅ Ingested chunk 251 with 50000 rows.\n",
      "✅ Ingested chunk 252 with 50000 rows.\n",
      "✅ Ingested chunk 253 with 50000 rows.\n",
      "✅ Ingested chunk 254 with 50000 rows.\n",
      "✅ Ingested chunk 255 with 50000 rows.\n",
      "✅ Ingested chunk 256 with 50000 rows.\n",
      "✅ Ingested chunk 257 with 25363 rows.\n"
     ]
    }
   ],
   "source": [
    " #✅ Config\n",
    "csv_file = \"C:\\\\Users\\\\Prathamesh\\\\Vendor Performance Analysis\\\\sales.csv\"   \n",
    "table_name = \"sales\"                \n",
    "chunk_size = 50000 \n",
    "\n",
    "\n",
    "#Logging \n",
    "logging.basicConfig(\n",
    "    filename=\"logs/chunk_ingestion.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    filemode='a'\n",
    ")\n",
    "\n",
    "try:\n",
    "    engine = create_engine(\"mysql+pymysql://root:root@localhost:3306/inventory\")  \n",
    "    conn = engine.connect()\n",
    "    logging.info(\"Connected to MySQL.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"MySQL connection failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ✅ Ingest in Chunks\n",
    "try:\n",
    "    i = 0\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        i += 1\n",
    "        try:\n",
    "            chunk.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "            logging.info(f\"Chunk {i} ingested successfully. Rows: {len(chunk)}\")\n",
    "            print(f\"✅ Ingested chunk {i} with {len(chunk)} rows.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to insert chunk {i}: {e}\")\n",
    "            print(f\"⚠️ Failed to insert chunk {i}. Error logged.\")\n",
    "except Exception as e:\n",
    "    logging.critical(f\"Fatal error during CSV ingestion: {e}\")\n",
    "    print(\"❌ Something went wrong. Check logs for details.\")\n",
    "finally:\n",
    "    conn.close()\n",
    "    logging.info(\"MySQL connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2ef17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
